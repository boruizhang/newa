{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf664268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d4f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tokens = util.get_labeled_tokens()\n",
    "train, test = util.make_train_test(labeled_tokens)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac3272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_bigram(tokens):\n",
    "    \"\"\"input is a list/iterable of tokens, output/generator is list of dictionary features, like\n",
    "    [{word_nm1: the, word_n: dog}]\n",
    "    if tok == <s>, word_nm1 = \"</s>\" (padding)\n",
    "    \"\"\"\n",
    "    prev = \"</S>\" # end of sentence marker\n",
    "    for tok, lab in tokens:\n",
    "        feature_dict = {}\n",
    "        feature_dict[\"word_n\"] = tok\n",
    "        feature_dict[\"word_nm1\"] = prev\n",
    "        prev = tok\n",
    "        yield feature_dict, lab\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6ee1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeat = list(featurize_bigram(train))\n",
    "testfeat = list(featurize_bigram(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c511cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'word_n': '<S>', 'word_nm1': '</S>'}, 'O'),\n",
       " ({'word_n': 'थः ', 'word_nm1': '<S>'}, 'B-EC'),\n",
       " ({'word_n': 'नं', 'word_nm1': 'थः '}, 'I-EC'),\n",
       " ({'word_n': 'छम्ह', 'word_nm1': 'नं'}, 'I-EC'),\n",
       " ({'word_n': 'शक्तिशाली', 'word_nm1': 'छम्ह'}, 'I-EC')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc45a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 711 tokens with 33 phrases; found: 113 phrases; correct: 8.\n",
      "accuracy:  71.87%; precision:   7.08%; recall:  24.24%; FB1:  10.96\n",
      "               EC: precision:   7.08%; recall:  24.24%; FB1:  10.96  113\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '113', '8', '71.87', '7.08', '24.24', '10.96')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "test_pred = list(map(classifier.classify, [tok for tok, lab in testfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61e30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_wordandtag_bigram(tokens, classify=False):\n",
    "    \"\"\"input is a list/iterable of tokens, output/generator is list of dictionary features, like\n",
    "    [{word_nm1: the, word_n: dog}]\n",
    "    if tok == <s>, word_nm1 = \"</s>\" (padding)\n",
    "    \"\"\"\n",
    "    prev_tok = \"</S>\" # end of sentence marker\n",
    "    prev_lab = \"O\"\n",
    "    for tok, lab in tokens:\n",
    "        feature_dict = {}\n",
    "        feature_dict[\"word_n\"] = tok\n",
    "        feature_dict[\"word_nm1\"] = prev_tok\n",
    "        feature_dict[\"lab_nm1\"] = prev_lab\n",
    "        prev_tok = tok\n",
    "        if classify: # this is the part that makes it honest fair, see below\n",
    "            lab = classify(feature_dict)\n",
    "        prev_lab = lab\n",
    "        yield feature_dict, lab\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06646977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3997 tokens with 168 phrases; found: 169 phrases; correct: 153.\n",
      "accuracy:  99.57%; precision:  90.53%; recall:  91.07%; FB1:  90.80\n",
      "               EC: precision:  90.53%; recall:  91.07%; FB1:  90.80  169\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 40 phrases; correct: 21.\n",
      "accuracy:  97.33%; precision:  52.50%; recall:  63.64%; FB1:  57.53\n",
      "               EC: precision:  52.50%; recall:  63.64%; FB1:  57.53  40\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '40', '21', '97.33', '52.50', '63.64', '57.53')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "testfeat = list(featurize_wordandtag_bigram(test))\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "test_pred = list(map(classifier.classify, [tok for tok, lab in testfeat]))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec42237",
   "metadata": {},
   "source": [
    "#### the previous is not honest b/c we consider knowing the true preceding takes, as opposed to the predicted preceding tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "015cccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3997 tokens with 168 phrases; found: 169 phrases; correct: 153.\n",
      "accuracy:  99.57%; precision:  90.53%; recall:  91.07%; FB1:  90.80\n",
      "               EC: precision:  90.53%; recall:  91.07%; FB1:  90.80  169\n",
      "\n",
      "processed 3997 tokens with 168 phrases; found: 163 phrases; correct: 153.\n",
      "accuracy:  97.25%; precision:  93.87%; recall:  91.07%; FB1:  92.45\n",
      "               EC: precision:  93.87%; recall:  91.07%; FB1:  92.45  163\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 33 phrases; correct: 19.\n",
      "accuracy:  84.39%; precision:  57.58%; recall:  57.58%; FB1:  57.58\n",
      "               EC: precision:  57.58%; recall:  57.58%; FB1:  57.58  33\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '33', '19', '84.39', '57.58', '57.58', '57.58')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "\n",
    "# this way predicts over-optimistically because the preceding tags/labels are known\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "# this way should be more fair/honest\n",
    "train_pred = list(featurize_wordandtag_bigram(train, classify=classifier.classify))\n",
    "util.conlleval(train_toks, train_true, [pred for _, pred in train_pred])\n",
    "\n",
    "# fair/honest for test\n",
    "test_pred = list(featurize_wordandtag_bigram(test, classify=classifier.classify))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, [pred for _, pred in test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c47c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                word_nm1 = 'धकाः'              O : B-EC   =     32.3 : 1.0\n",
      "                word_nm1 = 'धका'               O : I-EC   =     29.6 : 1.0\n",
      "                  word_n = 'धका'            I-EC : O      =     17.4 : 1.0\n",
      "                  word_n = 'धकाः'           I-EC : O      =     11.6 : 1.0\n",
      "                word_nm1 = '<S>'            B-EC : O      =     11.1 : 1.0\n",
      "                 lab_nm1 = 'I-EC'           I-EC : O      =     10.4 : 1.0\n",
      "                word_nm1 = 'नं'                O : B-EC   =      7.8 : 1.0\n",
      "                word_nm1 = 'हे'             I-EC : B-EC   =      7.7 : 1.0\n",
      "                  word_n = 'आदिवासी'        I-EC : O      =      7.0 : 1.0\n",
      "                  word_n = 'गुलि'           I-EC : O      =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995e222",
   "metadata": {},
   "source": [
    "धकाः - dhaka, dhakāḥ1  (irr. form of dhāye)  1. quotation marker: having said that etc.  2. (with dat.) for s.o.'s benefit\n",
    "\n",
    "नं - na/no? na5 part.  1. na ... na  denoting doubt, uncertainty: va na vayi na mavayi. Who knows whether he will come or not.  2. nearly, almost\n",
    "\n",
    "हे -\n",
    "haṂ1 interj., part.  1. yes, indeed  2. marker of reported speech: vaṃ chanta vā dhāla haṂ.  He told you to come, he said. 3. expression of surprise; haṂyā khaṂ  rumour\n",
    "haḥ3  interj.  expression of approval used by older people to younger people\n",
    "hu n.  wheat (goblins' language)\n",
    "huṂ1  interj.  yes, well\n",
    "he1 emph.part.  indeed, definitely, really: thuthāy jike dhebā he madu. Right here, I really don't have any money.\n",
    "\n",
    "आदिवासी - tribal? https://hi.wikipedia.org/wiki/%E0%A4%86%E0%A4%A6%E0%A4%BF%E0%A4%B5%E0%A4%BE%E0%A4%B8%E0%A5%80\n",
    "\n",
    "गुलि - how much https://www.chegg.com/flashcards/newar-phrases-c1d04688-212d-4ee4-a77b-b350cabdf6dc/deck\n",
    "gathe (var. gay) pron.  how; ~ki adv.  such as, for instance; ~khese mine  to feel uneasy; ~gathe adv.  how come, don't know (how it happened); ~jaka pron.  how; ~bhanaṃ adv. \\\n",
    " how much; ~yānā pron.  how; ~hana adv.  as expected\n",
    " gapāy pron.  1. how much, to what extent  2. to such an extent (cf. apāy, thapāy); ~cvaḥ pron.  to what extent; ~dhaṃ pron.  how big; ~hākaḥ pron.  how long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173178",
   "metadata": {},
   "source": [
    "### Try maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60352c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.479\n",
      "             2          -0.48843        0.922\n",
      "             3          -0.35555        0.981\n",
      "             4          -0.28188        0.987\n",
      "             5          -0.23479        0.990\n",
      "             6          -0.20185        0.991\n",
      "             7          -0.17739        0.994\n",
      "             8          -0.15846        0.995\n",
      "             9          -0.14335        0.996\n",
      "            10          -0.13098        0.997\n",
      "            11          -0.12067        0.997\n",
      "            12          -0.11192        0.997\n",
      "            13          -0.10442        0.997\n",
      "            14          -0.09790        0.997\n",
      "            15          -0.09218        0.997\n",
      "            16          -0.08712        0.998\n",
      "            17          -0.08262        0.998\n",
      "            18          -0.07857        0.998\n",
      "            19          -0.07493        0.998\n",
      "            20          -0.07162        0.998\n",
      "            21          -0.06861        0.998\n",
      "            22          -0.06585        0.998\n",
      "            23          -0.06332        0.998\n",
      "            24          -0.06098        0.998\n",
      "            25          -0.05882        0.998\n",
      "            26          -0.05682        0.998\n",
      "            27          -0.05496        0.998\n",
      "            28          -0.05322        0.998\n",
      "            29          -0.05159        0.998\n",
      "            30          -0.05007        0.998\n",
      "            31          -0.04864        0.998\n",
      "            32          -0.04729        0.998\n",
      "            33          -0.04602        0.998\n",
      "            34          -0.04482        0.998\n",
      "            35          -0.04368        0.998\n",
      "            36          -0.04261        0.998\n",
      "            37          -0.04159        0.998\n",
      "            38          -0.04062        0.998\n",
      "            39          -0.03970        0.998\n",
      "            40          -0.03882        0.998\n",
      "            41          -0.03798        0.998\n",
      "            42          -0.03718        0.999\n",
      "            43          -0.03642        0.999\n",
      "            44          -0.03569        0.999\n",
      "            45          -0.03499        0.999\n",
      "            46          -0.03431        0.999\n",
      "            47          -0.03367        0.999\n",
      "            48          -0.03305        0.999\n",
      "            49          -0.03245        0.999\n",
      "            50          -0.03188        0.999\n",
      "            51          -0.03133        0.999\n",
      "            52          -0.03080        0.999\n",
      "            53          -0.03029        0.999\n",
      "            54          -0.02979        0.999\n",
      "            55          -0.02932        0.999\n",
      "            56          -0.02886        0.999\n",
      "            57          -0.02841        0.999\n",
      "            58          -0.02798        0.999\n",
      "            59          -0.02756        0.999\n",
      "            60          -0.02716        0.999\n",
      "            61          -0.02677        0.999\n",
      "            62          -0.02639        0.999\n",
      "            63          -0.02602        0.999\n",
      "            64          -0.02567        0.999\n",
      "            65          -0.02532        0.999\n",
      "            66          -0.02498        0.999\n",
      "            67          -0.02466        0.999\n",
      "            68          -0.02434        0.999\n",
      "            69          -0.02403        0.999\n",
      "            70          -0.02373        0.999\n",
      "            71          -0.02344        0.999\n",
      "            72          -0.02315        0.999\n",
      "            73          -0.02288        0.999\n",
      "            74          -0.02261        0.999\n",
      "            75          -0.02234        0.999\n",
      "            76          -0.02209        0.999\n",
      "            77          -0.02184        0.999\n",
      "            78          -0.02159        0.999\n",
      "            79          -0.02135        0.999\n",
      "            80          -0.02112        0.999\n",
      "            81          -0.02089        0.999\n",
      "            82          -0.02067        0.999\n",
      "            83          -0.02046        0.999\n",
      "            84          -0.02024        0.999\n",
      "            85          -0.02004        0.999\n",
      "            86          -0.01983        0.999\n",
      "            87          -0.01964        0.999\n",
      "            88          -0.01944        0.999\n",
      "            89          -0.01925        0.999\n",
      "            90          -0.01907        0.999\n",
      "            91          -0.01889        0.999\n",
      "            92          -0.01871        0.999\n",
      "            93          -0.01853        0.999\n",
      "            94          -0.01836        0.999\n",
      "            95          -0.01820        0.999\n",
      "            96          -0.01803        0.999\n",
      "            97          -0.01787        0.999\n",
      "            98          -0.01771        0.999\n",
      "            99          -0.01756        0.999\n",
      "         Final          -0.01741        0.999\n",
      "processed 3997 tokens with 168 phrases; found: 170 phrases; correct: 164.\n",
      "accuracy:  99.90%; precision:  96.47%; recall:  97.62%; FB1:  97.04\n",
      "               EC: precision:  96.47%; recall:  97.62%; FB1:  97.04  170\n",
      "\n",
      "processed 3997 tokens with 168 phrases; found: 171 phrases; correct: 164.\n",
      "accuracy:  99.50%; precision:  95.91%; recall:  97.62%; FB1:  96.76\n",
      "               EC: precision:  95.91%; recall:  97.62%; FB1:  96.76  171\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 41 phrases; correct: 26.\n",
      "accuracy:  83.12%; precision:  63.41%; recall:  78.79%; FB1:  70.27\n",
      "               EC: precision:  63.41%; recall:  78.79%; FB1:  70.27  41\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '41', '26', '83.12', '63.41', '78.79', '70.27')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "classifier = nltk.MaxentClassifier.train(trainfeat)\n",
    "\n",
    "# this way predicts over-optimistically because the preceding tags/labels are known\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "# this way should be more fair/honest\n",
    "train_pred = list(featurize_wordandtag_bigram(train, classify=classifier.classify))\n",
    "util.conlleval(train_toks, train_true, [pred for _, pred in train_pred])\n",
    "\n",
    "# fair/honest for test\n",
    "test_pred = list(featurize_wordandtag_bigram(test, classify=classifier.classify))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, [pred for _, pred in test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a07828b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20.548 word_n=='न्ह्यथसें ' and label is 'B-EC'\n",
      "  14.491 word_n=='नेवाःत ' and label is 'B-EC'\n",
      "  11.732 word_n=='बुद्धजयन्ती ' and label is 'B-EC'\n",
      "  11.243 word_n==']तःगु' and label is 'I-EC'\n",
      " -10.890 word_nm1=='धकाः' and label is 'B-EC'\n",
      "  10.462 word_n=='छुं ' and label is 'B-EC'\n",
      "  10.364 word_n=='वय्कलं ' and label is 'B-EC'\n",
      "  -9.529 word_nm1=='धकाः' and label is 'I-EC'\n",
      "   9.171 word_nm1=='थहां' and label is 'O'\n",
      "   9.076 word_n=='आः ' and label is 'B-EC'\n",
      "   8.858 word_nm1=='नीस्वनेधुंकल' and label is 'O'\n",
      "   8.858 word_nm1=='दइमखु' and label is 'O'\n",
      "   8.858 word_nm1=='वइतिनि' and label is 'O'\n",
      "   8.549 word_nm1=='धका:' and label is 'O'\n",
      "   8.490 word_nm1=='न्ह्याःवःगु' and label is 'O'\n",
      "   8.325 word_n=='ख्यायेगु' and label is 'I-EC'\n",
      "   8.325 word_n==']दयेकूगु' and label is 'I-EC'\n",
      "   8.325 word_n==']न्हिया' and label is 'I-EC'\n",
      "   8.325 word_n==']उल्लेख' and label is 'I-EC'\n",
      "   8.325 word_n=='आधिकारीक' and label is 'I-EC'\n",
      "   8.325 word_n==']सीक' and label is 'I-EC'\n",
      "  -7.952 word_nm1=='धका' and label is 'I-EC'\n",
      "   7.763 word_n=='धर्मोदयसभां ' and label is 'B-EC'\n",
      "   7.738 word_n=='सामाजिक ' and label is 'B-EC'\n",
      "   7.697 word_nm1=='न्ह्यब्वइ' and label is 'O'\n",
      "   7.508 word_n=='महाद्यः ' and label is 'B-EC'\n",
      "   7.508 word_n=='ओम ' and label is 'B-EC'\n",
      "   7.508 word_n=='ताज़िक-ओ-तुर्क ' and label is 'B-EC'\n",
      "   7.393 word_n=='धकाः ' and label is 'B-EC'\n",
      "   7.187 word_n=='थुकियात' and label is 'I-EC'\n",
      "   6.737 word_n=='थ्व ' and label is 'B-EC'\n",
      "   6.728 word_n=='दक्ले ' and label is 'B-EC'\n",
      "   6.416 word_nm1=='ख' and label is 'O'\n",
      "   6.332 word_n=='नेपाः ' and label is 'B-EC'\n",
      "   6.332 word_n=='उकियालिसें ' and label is 'B-EC'\n",
      "   6.284 word_n=='सुना' and label is 'O'\n",
      "   6.284 word_nm1=='धका,' and label is 'O'\n",
      "   6.198 word_n=='नेवाः ' and label is 'B-EC'\n",
      "   6.065 word_n=='नसा' and label is 'O'\n",
      "   6.065 word_n=='थौंतकया' and label is 'O'\n",
      "   6.065 word_n=='स्कुलय्' and label is 'O'\n",
      "   6.065 word_n=='थुगुसिइ' and label is 'O'\n",
      "   6.065 word_n=='”' and label is 'O'\n",
      "   6.065 word_n=='अथे' and label is 'O'\n",
      "   6.065 word_n=='अझ' and label is 'O'\n",
      "   6.065 word_n=='लिपा,' and label is 'O'\n",
      "   6.065 word_n=='न्यनेदु' and label is 'O'\n",
      "   6.065 word_n=='अके' and label is 'O'\n",
      "   6.065 word_n=='यदि' and label is 'O'\n",
      "   6.065 word_n=='थःजन्मदाता' and label is 'O'\n",
      "   6.065 word_n=='धायेत्यनागु' and label is 'O'\n",
      "   6.065 word_n=='न्हूदँया' and label is 'O'\n",
      "   6.065 word_n=='4उकिं' and label is 'O'\n",
      "   6.065 word_n=='8अले' and label is 'O'\n",
      "   6.065 word_n=='तमिल' and label is 'O'\n",
      "   6.037 word_n=='थःगु ' and label is 'B-EC'\n",
      "   5.915 word_nm1=='वने' and label is 'O'\n",
      "   5.879 word_nm1=='धका' and label is 'O'\n",
      "   5.824 word_nm1=='मेगु ' and label is 'I-EC'\n",
      "   5.616 word_n=='स्वस्थानीया' and label is 'O'\n",
      "   5.610 word_nm1=='धाइ' and label is 'O'\n",
      "   5.583 word_n=='अझ ' and label is 'B-EC'\n",
      "   5.503 word_n=='अख्तियारया' and label is 'I-EC'\n",
      "   5.336 word_nm1=='वलकि' and label is 'B-EC'\n",
      "   5.309 word_nm1=='स्वयेबलय्' and label is 'B-EC'\n",
      "   5.309 word_nm1=='याम्ह' and label is 'B-EC'\n",
      "   5.261 word_n=='गूगु' and label is 'I-EC'\n",
      "   5.233 word_n=='थुकिं' and label is 'O'\n",
      "   5.231 word_n=='थः ' and label is 'B-EC'\n",
      "   5.231 word_n=='४७ ' and label is 'B-EC'\n",
      "   5.231 word_n=='सात ' and label is 'B-EC'\n",
      "   5.231 word_n=='सरकारया ' and label is 'B-EC'\n",
      "   5.231 word_n=='न्ह्याबलेंयात ' and label is 'B-EC'\n",
      "   5.231 word_n=='कन्हय् ' and label is 'B-EC'\n",
      "   5.231 word_n=='अथे ' and label is 'B-EC'\n",
      "   5.231 word_n=='जगतसुन्दर ' and label is 'B-EC'\n",
      "   5.231 word_n=='उगु ' and label is 'B-EC'\n",
      "   5.231 word_n=='स्वास्थानी ' and label is 'B-EC'\n",
      "   5.231 word_n=='माघ ' and label is 'B-EC'\n",
      "   5.231 word_n=='सक्वया ' and label is 'B-EC'\n",
      "   5.231 word_n=='सक्वय् ' and label is 'B-EC'\n",
      "   5.231 word_n=='स्वस्थानी ' and label is 'B-EC'\n",
      "   5.231 word_n=='तसकं ' and label is 'B-EC'\n",
      "   5.231 word_n=='भिम्सेन ' and label is 'B-EC'\n",
      "   5.231 word_n=='झिंनिलाया ' and label is 'B-EC'\n",
      "   5.231 word_n=='दक्वं ' and label is 'B-EC'\n",
      "   5.231 word_n=='” ' and label is 'B-EC'\n",
      "   5.231 word_n=='वं ' and label is 'B-EC'\n",
      "   5.231 word_n=='चिकुइगु ' and label is 'B-EC'\n",
      "   5.231 word_n=='वय्\\u200cकःयात ' and label is 'B-EC'\n",
      "   5.231 word_n=='व ' and label is 'B-EC'\n",
      "   5.231 word_n=='वंगु ' and label is 'B-EC'\n",
      "   5.231 word_n=='पिखा ' and label is 'B-EC'\n",
      "   5.231 word_n=='वहे ' and label is 'B-EC'\n",
      "   5.231 word_n=='अभिभ्रित ' and label is 'B-EC'\n",
      "   5.231 word_n=='थन ' and label is 'B-EC'\n",
      "   5.231 word_n=='चित्लाङ्गया ' and label is 'B-EC'\n",
      "   5.231 word_n=='थनया ' and label is 'B-EC'\n",
      "   5.231 word_n=='वंशावलिइ ' and label is 'B-EC'\n",
      "   5.231 word_n=='थुगु ' and label is 'B-EC'\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "न्ह्यथसें\n",
    "nhe-thane v.t.  1. to mention  2. to propose, to suggest, to raise a question: jiṃ thva khaṂ suṃ āju pākheṃ nhethane. I shall raise this matter with one of the elders.\n",
    "\n",
    "नेवाःत\n",
    "nevāḥ n.anim. (-vāla-)  Newar; ~ākhaḥ n. (-khala-)  Newari script; ~khaṂ n.  Newari language; ~jā n.  kind of `bali' offering: boiled rice rolled into a ball; ~bhāy n. (-bhāsa\\\n",
    "-)  Newari language\n",
    "\n",
    "बुद्धजयन्ती\n",
    "https://en.wikipedia.org/wiki/Vesak\n",
    "\n",
    "]तःगु\n",
    "? tegu\n",
    "\n",
    "धकाः\n",
    "dhaka\n",
    "\n",
    "छुं\n",
    "chu (var. chū) pron.  what, which: chu khaḥ?  How are you?  chaṃ chu bicāḥ thva khaṂy yānā?  What do you think about this?\n",
    "    \n",
    "वय्कलं\n",
    "? vayākathaṃ adv.  abruptly\n",
    "धकाः dhaka\n",
    "\n",
    "थहां\n",
    "thathe (var. thay) adv.  in this way\n",
    "\n",
    "thatheṃ adv.  immediately, instantly; ~he adv.  id.\n",
    "\n",
    "thatheka adv.  like this\n",
    "\n",
    "tha-thene v.i.  1. to reach, to approach the top  2. to get promoted\n",
    "\n",
    "thathe|-dakhāḥ adv.  exactly; ~bhanaṃ adv.  thus, in that way; ~he adv.  exactly like this\n",
    "\n",
    "आः\n",
    "aa\n",
    "\n",
    "नीस्वनेधुंकल\n",
    "? neesvanedhunkal\n",
    "\n",
    "दइमखु\n",
    "? daṂkaḥmi (var. dakaḥmi) n.anim.  mason, brick-layer; ~kā n. (-kā)  line of bricks; ~cupi n. (-pu)  tool with a blade for cutting and shaping bricks; ~jyā n.  masonry; ~nāyaḥ n\\\n",
    ".anim. (-yeka-)  foreman of masons\n",
    "\n",
    "वइतिनि\n",
    "धका:\n",
    "न्ह्याःवःगु"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b3ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 I-EC O नं\n",
      "435 I-EC O मोह\n",
      "577 O I-EC </S>\n",
      "578 O I-EC <S>\n",
      "579 O B-EC अथे\n",
      "580 O I-EC हे\n",
      "581 O I-EC सक्व\n",
      "582 O I-EC नाप\n",
      "583 O I-EC स्वापू\n",
      "584 O I-EC दुगु\n",
      "585 O I-EC ग्रन्थ\n",
      "586 O I-EC मणिशैल\n",
      "587 O I-EC महावरणय्\n",
      "588 O I-EC धाःसा\n",
      "813 O I-EC </S>\n",
      "814 O I-EC <S>\n",
      "3309 I-EC O हे\n",
      "3310 I-EC O स्वीकार\n",
      "3311 I-EC O मयात\n",
      "3312 I-EC O धाःसा\n"
     ]
    }
   ],
   "source": [
    "for i, (tr, pr, tok) in enumerate(zip(train_true, [pred for _, pred in train_pred], train_toks)):\n",
    "    if tr != pr or i:\n",
    "        print(i, tr, pr, tok)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c20aa71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "700-147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4503f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually the previous was not really bigram, only two word context here's a better bigram\n",
    "# using joint features\n",
    "def featurize_wordandtag_bigram2(tokens, classify=False):\n",
    "    \"\"\"input is a list/iterable of tokens, output/generator is list of dictionary features, like\n",
    "    [{word_nm1: the, word_n: dog}]\n",
    "    if tok == <s>, word_nm1 = \"</s>\" (padding)\n",
    "    \"\"\"\n",
    "    prev_tok = \"</S>\" # end of sentence marker\n",
    "    prev_lab = \"O\"\n",
    "    for tok, lab in tokens:\n",
    "        feature_dict = {}\n",
    "        feature_dict[\"word_n\"] = tok\n",
    "        feature_dict[\"word_n-1\"] = prev_tok\n",
    "        feature_dict[\"word_n-1,word_n\"] = prev_tok + \",\"  + tok\n",
    "        feature_dict[\"lab_n-1\"] = prev_lab\n",
    "        feature_dict[\"lab_n-1,word_n\"] = prev_lab + \",\"  + tok\n",
    "        feature_dict[\"lab_n-1,word_n-1\"] = prev_lab + \",\"  + prev_tok\n",
    "        feature_dict[\"lab_n-1,word_n-1,word_n\"] = prev_lab + \",\"  + prev_tok + \",\" + tok\n",
    "        prev_tok = tok\n",
    "        if classify: # this is the part that makes it honest fair, see below\n",
    "            lab = classify(feature_dict)\n",
    "        prev_lab = lab\n",
    "        yield feature_dict, lab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db23c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.479\n",
      "             2          -0.43712        0.951\n",
      "             3          -0.30547        0.997\n",
      "             4          -0.23673        0.998\n",
      "             5          -0.19374        0.999\n",
      "             6          -0.16415        0.999\n",
      "             7          -0.14249        0.999\n",
      "             8          -0.12594        0.999\n",
      "             9          -0.11287        0.999\n",
      "            10          -0.10229        0.999\n",
      "            11          -0.09354        0.999\n",
      "            12          -0.08618        0.999\n",
      "            13          -0.07991        0.999\n",
      "            14          -0.07450        0.999\n",
      "            15          -0.06978        0.999\n",
      "            16          -0.06563        0.999\n",
      "            17          -0.06195        0.999\n",
      "            18          -0.05867        0.999\n",
      "            19          -0.05573        0.999\n",
      "            20          -0.05307        0.999\n",
      "            21          -0.05065        0.999\n",
      "            22          -0.04845        0.999\n",
      "            23          -0.04644        0.999\n",
      "            24          -0.04458        0.999\n",
      "            25          -0.04288        0.999\n",
      "            26          -0.04130        0.999\n",
      "            27          -0.03983        0.999\n",
      "            28          -0.03847        0.999\n",
      "            29          -0.03720        0.999\n",
      "            30          -0.03601        0.999\n",
      "            31          -0.03490        0.999\n",
      "            32          -0.03386        0.999\n",
      "            33          -0.03287        0.999\n",
      "            34          -0.03195        0.999\n",
      "            35          -0.03107        0.999\n",
      "            36          -0.03025        0.999\n",
      "            37          -0.02947        0.999\n",
      "            38          -0.02872        0.999\n",
      "            39          -0.02802        0.999\n",
      "            40          -0.02735        0.999\n",
      "            41          -0.02671        0.999\n",
      "            42          -0.02610        0.999\n",
      "            43          -0.02552        0.999\n",
      "            44          -0.02497        0.999\n",
      "            45          -0.02444        0.999\n",
      "            46          -0.02393        0.999\n",
      "            47          -0.02345        0.999\n",
      "            48          -0.02298        0.999\n",
      "            49          -0.02253        0.999\n",
      "            50          -0.02210        0.999\n",
      "            51          -0.02169        0.999\n",
      "            52          -0.02129        0.999\n",
      "            53          -0.02091        0.999\n",
      "            54          -0.02054        0.999\n",
      "            55          -0.02018        0.999\n",
      "            56          -0.01984        0.999\n",
      "            57          -0.01951        0.999\n",
      "            58          -0.01919        0.999\n",
      "            59          -0.01888        0.999\n",
      "            60          -0.01858        0.999\n",
      "            61          -0.01829        0.999\n",
      "            62          -0.01801        0.999\n",
      "            63          -0.01774        0.999\n",
      "            64          -0.01748        0.999\n",
      "            65          -0.01722        0.999\n",
      "            66          -0.01698        0.999\n",
      "            67          -0.01674        0.999\n",
      "            68          -0.01650        0.999\n",
      "            69          -0.01628        0.999\n",
      "            70          -0.01606        0.999\n",
      "            71          -0.01584        0.999\n",
      "            72          -0.01564        0.999\n",
      "            73          -0.01543        0.999\n",
      "            74          -0.01524        0.999\n",
      "            75          -0.01505        0.999\n",
      "            76          -0.01486        0.999\n",
      "            77          -0.01468        0.999\n",
      "            78          -0.01450        0.999\n",
      "            79          -0.01433        0.999\n",
      "            80          -0.01416        0.999\n",
      "            81          -0.01399        0.999\n",
      "            82          -0.01383        0.999\n",
      "            83          -0.01368        0.999\n",
      "            84          -0.01352        0.999\n",
      "            85          -0.01337        0.999\n",
      "            86          -0.01323        0.999\n",
      "            87          -0.01309        0.999\n",
      "            88          -0.01295        0.999\n",
      "            89          -0.01281        0.999\n",
      "            90          -0.01268        0.999\n",
      "            91          -0.01255        0.999\n",
      "            92          -0.01242        0.999\n",
      "            93          -0.01229        0.999\n",
      "            94          -0.01217        0.999\n",
      "            95          -0.01205        0.999\n",
      "            96          -0.01193        0.999\n",
      "            97          -0.01182        0.999\n",
      "            98          -0.01171        0.999\n",
      "            99          -0.01160        0.999\n",
      "         Final          -0.01149        0.999\n",
      "processed 3997 tokens with 168 phrases; found: 170 phrases; correct: 166.\n",
      "accuracy:  99.95%; precision:  97.65%; recall:  98.81%; FB1:  98.22\n",
      "               EC: precision:  97.65%; recall:  98.81%; FB1:  98.22  170\n",
      "\n",
      "processed 3997 tokens with 168 phrases; found: 170 phrases; correct: 166.\n",
      "accuracy:  99.85%; precision:  97.65%; recall:  98.81%; FB1:  98.22\n",
      "               EC: precision:  97.65%; recall:  98.81%; FB1:  98.22  170\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 37 phrases; correct: 27.\n",
      "accuracy:  89.73%; precision:  72.97%; recall:  81.82%; FB1:  77.14\n",
      "               EC: precision:  72.97%; recall:  81.82%; FB1:  77.14  37\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '37', '27', '89.73', '72.97', '81.82', '77.14')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try again\n",
    "trainfeat = list(featurize_wordandtag_bigram2(train))\n",
    "classifier = nltk.MaxentClassifier.train(trainfeat)\n",
    "\n",
    "# this way predicts over-optimistically because the preceding tags/labels are known\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "# this way should be more fair/honest\n",
    "train_pred = list(featurize_wordandtag_bigram2(train, classify=classifier.classify))\n",
    "util.conlleval(train_toks, train_true, [pred for _, pred in train_pred])\n",
    "\n",
    "# fair/honest for test\n",
    "test_pred = list(featurize_wordandtag_bigram2(test, classify=classifier.classify))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, [pred for _, pred in test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d60ed71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -9.862 word_n-1=='धकाः' and label is 'B-EC'\n",
      "  -4.649 word_n-1=='नं' and label is 'B-EC'\n",
      "  -4.252 word_n-1=='हे' and label is 'B-EC'\n",
      "  -4.170 word_n-1=='धकाः' and label is 'I-EC'\n",
      "   4.103 word_n-1,word_n=='हे,थ्व ' and label is 'B-EC'\n",
      "   4.103 lab_n-1,word_n-1,word_n=='O,हे,थ्व ' and label is 'B-EC'\n",
      "   3.972 word_n=='न्ह्यथसें ' and label is 'B-EC'\n",
      "   3.972 word_n-1,word_n=='धकाः,न्ह्यथसें ' and label is 'B-EC'\n",
      "   3.972 lab_n-1,word_n=='O,न्ह्यथसें ' and label is 'B-EC'\n",
      "   3.972 lab_n-1,word_n-1,word_n=='O,धकाः,न्ह्यथसें ' and label is 'B-EC'\n",
      "  -3.705 lab_n-1,word_n-1=='I-EC,धकाः' and label is 'I-EC'\n",
      "   3.677 word_n-1,word_n=='हे,वय्कलं ' and label is 'B-EC'\n",
      "   3.677 lab_n-1,word_n-1,word_n=='O,हे,वय्कलं ' and label is 'B-EC'\n",
      "   3.442 lab_n-1,word_n-1,word_n=='I-EC,दु,</S>' and label is 'O'\n",
      "  -3.429 lab_n-1,word_n=='I-EC,हे' and label is 'O'\n",
      "   3.392 word_n=='नेवाःत ' and label is 'B-EC'\n",
      "   3.392 word_n-1,word_n=='नं,नेवाःत ' and label is 'B-EC'\n",
      "   3.392 lab_n-1,word_n=='O,नेवाःत ' and label is 'B-EC'\n",
      "   3.392 lab_n-1,word_n-1,word_n=='O,नं,नेवाःत ' and label is 'B-EC'\n",
      "  -3.246 word_n-1=='धका' and label is 'I-EC'\n",
      "   3.101 lab_n-1,word_n-1=='O,धकाः' and label is 'B-EC'\n",
      "   3.085 word_n-1,word_n=='धकाः,थुकियात' and label is 'I-EC'\n",
      "   3.085 lab_n-1,word_n-1,word_n=='I-EC,धकाः,थुकियात' and label is 'I-EC'\n",
      "   2.993 lab_n-1,word_n-1,word_n=='O,खः,धकाः' and label is 'O'\n",
      "   2.908 word_n==']तःगु' and label is 'I-EC'\n",
      "   2.908 word_n-1,word_n=='धका,]तःगु' and label is 'I-EC'\n",
      "   2.908 lab_n-1,word_n=='I-EC,]तःगु' and label is 'I-EC'\n",
      "   2.908 lab_n-1,word_n-1,word_n=='I-EC,धका,]तःगु' and label is 'I-EC'\n",
      "   2.844 word_n-1,word_n=='वने,</S>' and label is 'O'\n",
      "   2.844 lab_n-1,word_n-1,word_n=='I-EC,वने,</S>' and label is 'O'\n",
      "  -2.746 lab_n-1,word_n-1=='I-EC,धका' and label is 'I-EC'\n",
      "   2.644 word_n=='बुद्धजयन्ती ' and label is 'B-EC'\n",
      "   2.644 word_n-1,word_n=='आः,बुद्धजयन्ती ' and label is 'B-EC'\n",
      "   2.644 lab_n-1,word_n=='O,बुद्धजयन्ती ' and label is 'B-EC'\n",
      "   2.644 lab_n-1,word_n-1,word_n=='O,आः,बुद्धजयन्ती ' and label is 'B-EC'\n",
      "  -2.634 lab_n-1=='I-EC' and label is 'O'\n",
      "   2.572 word_n=='ख्यायेगु' and label is 'I-EC'\n",
      "   2.572 word_n-1,word_n=='धकाः,ख्यायेगु' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n=='I-EC,ख्यायेगु' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n-1,word_n=='I-EC,धकाः,ख्यायेगु' and label is 'I-EC'\n",
      "   2.572 word_n==']दयेकूगु' and label is 'I-EC'\n",
      "   2.572 word_n-1,word_n=='धकाः,]दयेकूगु' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n=='I-EC,]दयेकूगु' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n-1,word_n=='I-EC,धकाः,]दयेकूगु' and label is 'I-EC'\n",
      "   2.572 word_n==']न्हिया' and label is 'I-EC'\n",
      "   2.572 word_n-1,word_n=='धकाः,]न्हिया' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n=='I-EC,]न्हिया' and label is 'I-EC'\n",
      "   2.572 lab_n-1,word_n-1,word_n=='I-EC,धकाः,]न्हिया' and label is 'I-EC'\n",
      "   2.572 word_n==']उल्लेख' and label is 'I-EC'\n",
      "   2.572 word_n-1,word_n=='धकाः,]उल्लेख' and label is 'I-EC'\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c9ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
