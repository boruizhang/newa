{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf664268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d4f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tokens = util.get_labeled_tokens()\n",
    "train, test = util.make_train_test(labeled_tokens)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aac2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_bigram(tokens):\n",
    "    \"\"\"input is a list/iterable of tokens, output/generator is list of dictionary features, like\n",
    "    [{word_nm1: the, word_n: dog}]\n",
    "    if tok == <s>, word_nm1 = \"</s>\" (padding)\n",
    "    \"\"\"\n",
    "    prev = \"</S>\" # end of sentence marker\n",
    "    for tok, lab in tokens:\n",
    "        feature_dict = {}\n",
    "        feature_dict[\"word_n\"] = tok\n",
    "        feature_dict[\"word_nm1\"] = prev\n",
    "        prev = tok\n",
    "        yield feature_dict, lab\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7754eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeat = list(featurize_bigram(train))\n",
    "testfeat = list(featurize_bigram(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f372f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'word_n': '<S>', 'word_nm1': '</S>'}, 'O'),\n",
       " ({'word_n': 'थः ', 'word_nm1': '<S>'}, 'B-EC'),\n",
       " ({'word_n': 'नं', 'word_nm1': 'थः '}, 'I-EC'),\n",
       " ({'word_n': 'छम्ह', 'word_nm1': 'नं'}, 'I-EC'),\n",
       " ({'word_n': 'शक्तिशाली', 'word_nm1': 'छम्ह'}, 'I-EC')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc45a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 711 tokens with 33 phrases; found: 113 phrases; correct: 8.\n",
      "accuracy:  71.87%; precision:   7.08%; recall:  24.24%; FB1:  10.96\n",
      "               EC: precision:   7.08%; recall:  24.24%; FB1:  10.96  113\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '113', '8', '71.87', '7.08', '24.24', '10.96')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "test_pred = list(map(classifier.classify, [tok for tok, lab in testfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d34d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_wordandtag_bigram(tokens, classify=False):\n",
    "    \"\"\"input is a list/iterable of tokens, output/generator is list of dictionary features, like\n",
    "    [{word_nm1: the, word_n: dog}]\n",
    "    if tok == <s>, word_nm1 = \"</s>\" (padding)\n",
    "    \"\"\"\n",
    "    prev_tok = \"</S>\" # end of sentence marker\n",
    "    prev_lab = \"O\"\n",
    "    for tok, lab in tokens:\n",
    "        feature_dict = {}\n",
    "        feature_dict[\"word_n\"] = tok\n",
    "        feature_dict[\"word_nm1\"] = prev_tok\n",
    "        feature_dict[\"lab_nm1\"] = prev_lab\n",
    "        prev_tok = tok\n",
    "        if classify: # this is the part that makes it honest fair, see below\n",
    "            lab = classify(feature_dict)\n",
    "        prev_lab = lab\n",
    "        yield feature_dict, lab\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba9a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3997 tokens with 168 phrases; found: 169 phrases; correct: 153.\n",
      "accuracy:  99.57%; precision:  90.53%; recall:  91.07%; FB1:  90.80\n",
      "               EC: precision:  90.53%; recall:  91.07%; FB1:  90.80  169\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 40 phrases; correct: 21.\n",
      "accuracy:  97.33%; precision:  52.50%; recall:  63.64%; FB1:  57.53\n",
      "               EC: precision:  52.50%; recall:  63.64%; FB1:  57.53  40\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '40', '21', '97.33', '52.50', '63.64', '57.53')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "testfeat = list(featurize_wordandtag_bigram(test))\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "test_pred = list(map(classifier.classify, [tok for tok, lab in testfeat]))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a49208",
   "metadata": {},
   "source": [
    "#### the previous is not honest b/c we consider knowing the true preceding takes, as opposed to the predicted preceding tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724145ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3997 tokens with 168 phrases; found: 169 phrases; correct: 153.\n",
      "accuracy:  99.57%; precision:  90.53%; recall:  91.07%; FB1:  90.80\n",
      "               EC: precision:  90.53%; recall:  91.07%; FB1:  90.80  169\n",
      "\n",
      "processed 3997 tokens with 168 phrases; found: 163 phrases; correct: 153.\n",
      "accuracy:  97.25%; precision:  93.87%; recall:  91.07%; FB1:  92.45\n",
      "               EC: precision:  93.87%; recall:  91.07%; FB1:  92.45  163\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 33 phrases; correct: 19.\n",
      "accuracy:  84.39%; precision:  57.58%; recall:  57.58%; FB1:  57.58\n",
      "               EC: precision:  57.58%; recall:  57.58%; FB1:  57.58  33\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '33', '19', '84.39', '57.58', '57.58', '57.58')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainfeat)\n",
    "\n",
    "# this way predicts over-optimistically because the preceding tags/labels are known\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "# this way should be more fair/honest\n",
    "train_pred = list(featurize_wordandtag_bigram(train, classify=classifier.classify))\n",
    "util.conlleval(train_toks, train_true, [pred for _, pred in train_pred])\n",
    "\n",
    "# fair/honest for test\n",
    "test_pred = list(featurize_wordandtag_bigram(test, classify=classifier.classify))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, [pred for _, pred in test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa1ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                word_nm1 = 'धकाः'              O : B-EC   =     32.3 : 1.0\n",
      "                word_nm1 = 'धका'               O : I-EC   =     29.6 : 1.0\n",
      "                  word_n = 'धका'            I-EC : O      =     17.4 : 1.0\n",
      "                  word_n = 'धकाः'           I-EC : O      =     11.6 : 1.0\n",
      "                word_nm1 = '<S>'            B-EC : O      =     11.1 : 1.0\n",
      "                 lab_nm1 = 'I-EC'           I-EC : O      =     10.4 : 1.0\n",
      "                word_nm1 = 'नं'                O : B-EC   =      7.8 : 1.0\n",
      "                word_nm1 = 'हे'             I-EC : B-EC   =      7.7 : 1.0\n",
      "                  word_n = 'आदिवासी'        I-EC : O      =      7.0 : 1.0\n",
      "                  word_n = 'गुलि'           I-EC : O      =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606b5e7",
   "metadata": {},
   "source": [
    "### Try maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfe08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.479\n",
      "             2          -0.48843        0.922\n",
      "             3          -0.35555        0.981\n",
      "             4          -0.28188        0.987\n",
      "             5          -0.23479        0.990\n",
      "             6          -0.20185        0.991\n",
      "             7          -0.17739        0.994\n",
      "             8          -0.15846        0.995\n",
      "             9          -0.14335        0.996\n",
      "            10          -0.13098        0.997\n",
      "            11          -0.12067        0.997\n",
      "            12          -0.11192        0.997\n",
      "            13          -0.10442        0.997\n",
      "            14          -0.09790        0.997\n",
      "            15          -0.09218        0.997\n",
      "            16          -0.08712        0.998\n",
      "            17          -0.08262        0.998\n",
      "            18          -0.07857        0.998\n",
      "            19          -0.07493        0.998\n",
      "            20          -0.07162        0.998\n",
      "            21          -0.06861        0.998\n",
      "            22          -0.06585        0.998\n",
      "            23          -0.06332        0.998\n",
      "            24          -0.06098        0.998\n",
      "            25          -0.05882        0.998\n",
      "            26          -0.05682        0.998\n",
      "            27          -0.05496        0.998\n",
      "            28          -0.05322        0.998\n",
      "            29          -0.05159        0.998\n",
      "            30          -0.05007        0.998\n",
      "            31          -0.04864        0.998\n",
      "            32          -0.04729        0.998\n",
      "            33          -0.04602        0.998\n",
      "            34          -0.04482        0.998\n",
      "            35          -0.04368        0.998\n",
      "            36          -0.04261        0.998\n",
      "            37          -0.04159        0.998\n",
      "            38          -0.04062        0.998\n",
      "            39          -0.03970        0.998\n",
      "            40          -0.03882        0.998\n",
      "            41          -0.03798        0.998\n",
      "            42          -0.03718        0.999\n",
      "            43          -0.03642        0.999\n",
      "            44          -0.03569        0.999\n",
      "            45          -0.03499        0.999\n",
      "            46          -0.03431        0.999\n",
      "            47          -0.03367        0.999\n",
      "            48          -0.03305        0.999\n",
      "            49          -0.03245        0.999\n",
      "            50          -0.03188        0.999\n",
      "            51          -0.03133        0.999\n",
      "            52          -0.03080        0.999\n",
      "            53          -0.03029        0.999\n",
      "            54          -0.02979        0.999\n",
      "            55          -0.02932        0.999\n",
      "            56          -0.02886        0.999\n",
      "            57          -0.02841        0.999\n",
      "            58          -0.02798        0.999\n",
      "            59          -0.02756        0.999\n",
      "            60          -0.02716        0.999\n",
      "            61          -0.02677        0.999\n",
      "            62          -0.02639        0.999\n",
      "            63          -0.02602        0.999\n",
      "            64          -0.02567        0.999\n",
      "            65          -0.02532        0.999\n",
      "            66          -0.02498        0.999\n",
      "            67          -0.02466        0.999\n",
      "            68          -0.02434        0.999\n",
      "            69          -0.02403        0.999\n",
      "            70          -0.02373        0.999\n",
      "            71          -0.02344        0.999\n",
      "            72          -0.02315        0.999\n",
      "            73          -0.02288        0.999\n",
      "            74          -0.02261        0.999\n",
      "            75          -0.02234        0.999\n",
      "            76          -0.02209        0.999\n",
      "            77          -0.02184        0.999\n",
      "            78          -0.02159        0.999\n",
      "            79          -0.02135        0.999\n",
      "            80          -0.02112        0.999\n",
      "            81          -0.02089        0.999\n",
      "            82          -0.02067        0.999\n",
      "            83          -0.02046        0.999\n",
      "            84          -0.02024        0.999\n",
      "            85          -0.02004        0.999\n",
      "            86          -0.01983        0.999\n",
      "            87          -0.01964        0.999\n",
      "            88          -0.01944        0.999\n",
      "            89          -0.01925        0.999\n",
      "            90          -0.01907        0.999\n",
      "            91          -0.01889        0.999\n",
      "            92          -0.01871        0.999\n",
      "            93          -0.01853        0.999\n",
      "            94          -0.01836        0.999\n",
      "            95          -0.01820        0.999\n",
      "            96          -0.01803        0.999\n",
      "            97          -0.01787        0.999\n",
      "            98          -0.01771        0.999\n",
      "            99          -0.01756        0.999\n",
      "         Final          -0.01741        0.999\n",
      "processed 3997 tokens with 168 phrases; found: 170 phrases; correct: 164.\n",
      "accuracy:  99.90%; precision:  96.47%; recall:  97.62%; FB1:  97.04\n",
      "               EC: precision:  96.47%; recall:  97.62%; FB1:  97.04  170\n",
      "\n",
      "processed 3997 tokens with 168 phrases; found: 171 phrases; correct: 164.\n",
      "accuracy:  99.50%; precision:  95.91%; recall:  97.62%; FB1:  96.76\n",
      "               EC: precision:  95.91%; recall:  97.62%; FB1:  96.76  171\n",
      "\n",
      "processed 711 tokens with 33 phrases; found: 41 phrases; correct: 26.\n",
      "accuracy:  83.12%; precision:  63.41%; recall:  78.79%; FB1:  70.27\n",
      "               EC: precision:  63.41%; recall:  78.79%; FB1:  70.27  41\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('711', '33', '41', '26', '83.12', '63.41', '78.79', '70.27')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainfeat = list(featurize_wordandtag_bigram(train))\n",
    "classifier = nltk.MaxentClassifier.train(trainfeat)\n",
    "\n",
    "# this way predicts over-optimistically because the preceding tags/labels are known\n",
    "train_pred = list(map(classifier.classify, [tok for tok, lab in trainfeat]))\n",
    "train_toks, train_true = zip(*train)\n",
    "util.conlleval(train_toks, train_true, train_pred)\n",
    "\n",
    "# this way should be more fair/honest\n",
    "train_pred = list(featurize_wordandtag_bigram(train, classify=classifier.classify))\n",
    "util.conlleval(train_toks, train_true, [pred for _, pred in train_pred])\n",
    "\n",
    "# fair/honest for test\n",
    "test_pred = list(featurize_wordandtag_bigram(test, classify=classifier.classify))\n",
    "test_toks, test_true = zip(*test)\n",
    "util.conlleval(test_toks, test_true, [pred for _, pred in test_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7352bf85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceedfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
